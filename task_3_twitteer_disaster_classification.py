# -*- coding: utf-8 -*-
"""Task-3 twitteer_disaster_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17iABinksTo3gWVlnp7jVxYHEINUf21Df
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string, math, os, re, time, random

from tqdm import tqdm

# visualizations
plt.style.use('ggplot')

# natural language processing
from collections import defaultdict
import wordcloud

# ignore warnings because they are annoying
import warnings
warnings.filterwarnings('ignore')

# for neural nets
import tensorflow as tf

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/twitter_disaster_task3 (2).csv')

"""# Part 3: Model Evaluation and Validation

Task: Model Evaluation

● Evaluate the trained models using appropriate evaluation metrics for binary classification tasks, such as accuracy, precision, recall, and F1-score.
"""

df.head()

from sklearn.metrics import accuracy_score, f1_score
def flat_accuracy(preds, labels):

    """A function for calculating accuracy scores"""

    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    return accuracy_score(labels_flat, pred_flat)

def flat_f1(preds, labels):

    """A function for calculating f1 scores"""

    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    return f1_score(labels_flat, pred_flat)

df.columns

# dropping unnecessary columns
df.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, inplace=True)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Assuming X contains your text data and y contains the corresponding labels
X = df['tokenized']
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Replace NaN values in X_train and X_test with an empty string
X_train = X_train.fillna('')
X_test = X_test.fillna('')

# Initialize TfidfVectorizer for TF-IDF representation
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Initialize and train the RandomForestClassifier model
model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
preds = model.predict(X_test_tfidf)

# Evaluate model performance
accuracy = accuracy_score(y_test, preds)
print("Accuracy:", accuracy)

report = classification_report(y_test, preds)
print("Classification Report:")
print(report)

"""# ● Visualize the performance metrics using confusion matrices, ROC curves, and precision-recall curves.

Confusion Matrix

A confusion matrix is a useful visualization for understanding the performance of a classification model. It displays the counts of true positives, false positives, true negatives, and false negatives
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
cm = confusion_matrix(y_test, preds)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# ROC Curve and AUC Score

The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier. It visualizes the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity)
"""

from sklearn.metrics import roc_curve, roc_auc_score

# Compute ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, preds)
auc_score = roc_auc_score(y_test, preds)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='blue')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# Precision-Recall Curve

The precision-recall curve plots precision (positive predictive value) against recall (sensitivity) at different threshold values. It is especially useful for imbalanced datasets where the class of interest is rare
"""

from sklearn.metrics import precision_recall_curve, average_precision_score

# Compute precision-recall curve and average precision score
precision, recall, _ = precision_recall_curve(y_test, preds)
average_precision = average_precision_score(y_test, preds)

# Plot precision-recall curve
plt.figure(figsize=(8, 6))
plt.step(recall, precision, where='post', label=f'AP = {average_precision:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

"""# These are the graphs of Confusion Matrix, ROC Curve and Precision-Recall Curve where AUC comes out to be 0.77 and AP is 0.67

● Compare the performance of different models to select the best-performing one for deployment.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

# Assuming 'X' contains the text data and 'y' contains the target labels
import numpy as np
import pandas as pd

# Vectorization
vectorizer = CountVectorizer()

# Convert X to string type and replace NaN with empty string
X = X.astype(str).fillna('') # Convert to string type and replace NaN with empty string to avoid the error

X_vectorized = vectorizer.fit_transform(X)

# Step 4: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

# Define a list of models to evaluate
models = {
    'Random Forest': RandomForestClassifier(),
    'Logistic Regression': LogisticRegression(),
    'Support Vector Machine': SVC(),
    'Multi-layer Perception': MLPClassifier()
}

# Evaluate each model using cross-validation or a hold-out test set
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {'Accuracy': accuracy}

# Display evaluation results
results_df = pd.DataFrame(results)
print(results_df)

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer
import re

#Function to clean text
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = text.lower()  # Convert text to lowercase
    return text

# Pipeline for text preprocessing and vectorization
pipeline = make_pipeline(
    FunctionTransformer(func=lambda x: x.astype(str), validate=False),  # Ensure input is string
    FunctionTransformer(func=lambda x: [clean_text(text) for text in x], validate=False),  # Clean text
    CountVectorizer()  # Convert text to word counts
)

# Preprocess text data
X_processed = pipeline.fit_transform(df['tokenized'])

print(X_processed.toarray())  # Print the processed text data

# Define a list of models to evaluate
models = {
    'Random Forest': RandomForestClassifier(),
    'Logistic Regression': LogisticRegression(),
    'Support Vector Machine': SVC(),
    'Multi-layer Perception': MLPClassifier()
}

# Train-test split with preprocessed text data
X_train_processed, X_test_processed, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# Evaluate each model using cross-validation or a hold-out test set
results = {}
for name, model in models.items():
    model.fit(X_train_processed, y_train)
    y_pred = model.predict(X_test_processed)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)

    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1,
        'ROC AUC': roc_auc
    }

# Display evaluation results
results_df = pd.DataFrame(results)
print(results_df)

"""#Task: Model Validation

● Validate the selected model on the testing dataset to assess its generalization ability.
"""

# Assuming 'LogisticRegression' is the selected model based on evaluation results

# Retrain the logistic regression model on the entire training dataset
logistic_regression_model = LogisticRegression()
logistic_regression_model.fit(X_train_processed, y_train)

# Predict labels for the testing dataset
y_pred_test = logistic_regression_model.predict(X_test_processed)

# Evaluate performance on the testing dataset
accuracy_test = accuracy_score(y_test, y_pred_test)
precision_test = precision_score(y_test, y_pred_test)
recall_test = recall_score(y_test, y_pred_test)
f1_test = f1_score(y_test, y_pred_test)
roc_auc_test = roc_auc_score(y_test, y_pred_test)

# Display performance metrics on the testing dataset
print("Testing Dataset Performance:")
print("Accuracy:", accuracy_test)
print("Precision:", precision_test)
print("Recall:", recall_test)
print("F1-score:", f1_test)
print("ROC AUC:", roc_auc_test)

"""#● Check for potential issues like overfitting or underfitting and adjust the model if necessary."""

# Adjust regularization parameter to address overfitting
logistic_regression_model = LogisticRegression(C=0.1)  # Adjust C value as needed

# Retrain the logistic regression model with adjusted parameters
logistic_regression_model.fit(X_train_processed, y_train)

# Predict labels for the testing dataset
y_pred_test = logistic_regression_model.predict(X_test_processed)

# Evaluate performance on the testing dataset
# Display performance metrics on the testing dataset

# Calculate performance metrics
accuracy_test = accuracy_score(y_test, y_pred_test)
precision_test = precision_score(y_test, y_pred_test)
recall_test = recall_score(y_test, y_pred_test)
f1_test = f1_score(y_test, y_pred_test)
roc_auc_test = roc_auc_score(y_test, y_pred_test)

# Create a bar chart
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']
values = [accuracy_test, precision_test, recall_test, f1_test, roc_auc_test]

plt.figure(figsize=(10, 6))
plt.bar(metrics, values, color='green')
plt.title('Performance Metrics on Testing Dataset')
plt.xlabel('Metric')
plt.ylabel('Score')
plt.ylim(0, 1)  # Set y-axis limit to [0, 1] for better visualization
plt.show()

# Create a DataFrame for performance metrics
results_dict = {
    'Metric': metrics,
    'Score': values
}
results_df = pd.DataFrame(results_dict)

# Display the DataFrame
print(results_df)

"""#  Ensure that the model performs well on unseen data and is robust to variations in the tweet text."""

with open('logistic_regression_model.pkl', 'rb') as file:
 regression_model = pickle.load(file)

# Assuming 'input_text' contains the input text you want to test
input_text = "This is a test input text."

# Step 1: Preprocess the input text (assuming the same preprocessing steps as during training)
# Step 2: Vectorize the input text using the same vectorizer instance used during training
input_text_vectorized = vectorizer.transform([input_text])

# Step 3: Make predictions on the vectorized input text
prediction = logistic_regression_model.predict(input_text_vectorized)

# Step 4: Interpret the prediction
if prediction == 1:
    print("The input text is predicted to be a disaster.")
else:
    print("The input text is not predicted to be a disaster.")

# Assuming 'input_text' contains the input text you want to test
input_text = "hello how are you."

# Step 1: Preprocess the input text (assuming the same preprocessing steps as during training)
# Step 2: Vectorize the input text using the same vectorizer instance used during training
input_text_vectorized = vectorizer.transform([input_text])

# Step 3: Make predictions on the vectorized input text
prediction = logistic_regression_model.predict(input_text_vectorized)

# Step 4: Interpret the prediction
if prediction == 1:
    print("The input text is predicted to be a disaster.")
else:
    print("The input text is not predicted to be a disaster.")

sns.heatmap(df.corr(numeric_only=True), cmap='coolwarm')

"""#Conclusion: By successfully developing and deploying a machine learning model for classifying disaster-related tweets, we can enhance the ability to detect and respond to emergencies more efficiently. This project represents a valuable opportunity to leverage NLP techniques for the greater good, contributing to the advancement of disaster management and public safety efforts."""

# Compute the correlation matrix
correlation_matrix = df.corr(numeric_only=True)

# Plot the heatmap
plt.figure(figsize=(10, 8))  # Adjust the figure size
sns.heatmap(
    correlation_matrix,
    annot=True,  # Display correlation coefficients
    fmt=".2f",   # Format the coefficients to 2 decimal places
    cmap='coolwarm',  # Colormap
    linewidths=0.5,   # Add grid lines between cells
    square=True,      # Make cells square
    cbar_kws={"shrink": 0.8, "label": "Correlation"},  # Customize color bar
)

# Add a title
plt.title("Correlation Matrix Heatmap", fontsize=16)
plt.show()