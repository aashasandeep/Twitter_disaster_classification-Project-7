# -*- coding: utf-8 -*-
"""Task -1 twitter_disaster_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PQKdY_EAmrxMn2vxHzhoCyu_MI8qm6GJ

#Task: Data Exploration
How to Achieve: - Load the dataset of 10,000 hand-classified tweets. - Explore the dataset&#39;s structure using Python libraries like Pandas to understand the
columns and data types. - Visualize the distribution of classes (disaster vs. non-disaster tweets) using histograms or
bar plots. - Analyze the frequency of keywords and phrases associated with disaster tweets.

### **NLP Project for Disaster Tweet Classification**

#**Importing** **liabraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string, math, os, re, time, random

from tqdm import tqdm

# visualizations
plt.style.use('ggplot')

# natural language processing
from collections import defaultdict
import wordcloud

# ignore warnings because they are annoying
import warnings
warnings.filterwarnings('ignore')

# for neural nets
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

"""#Reading Dataset

● Load the dataset of 10,000 hand-classified tweets
"""

df = pd.read_csv('/content/twitter_disaster.csv')

"""# Part 1: Data Exploration and Preparation

Task: Data Exploration


"""

df.head()# head of the dataset

df.tail()# tail of the dataset

print(f"This dataset has {df.shape[0]} rows and {df.shape[1]} columns")

df.describe()# describe the dataset

df.info()# get the information of the data

df.duplicated().sum()# get the duplicated dataset

df.isnull().sum()# get the missing value of the dataset

df.dtypes# find the types of the data

import nltk
import re
from nltk.corpus import stopwords # Import the stopwords corpus
from nltk.stem import PorterStemmer

# Download stopwords if you haven't already
nltk.download('stopwords')

stop = stopwords.words('english')
stemmer = PorterStemmer()

def clean_text(text):
    processed_text = re.sub(r"(@\[A-Za-z0-9]+)|([^A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", text)
    processed_text = processed_text.lower()
    processed_text = [word for word in processed_text.split() if word not in (stop)]
    processed_text = " ".join([stemmer.stem(word) for word in processed_text])
    return processed_text

def preprocessing(df):
    df.keyword.fillna("", inplace=True)
    df.location.fillna("", inplace=True)
    df.text = df.keyword + df.location + df.text
    df.text = df.text.apply(lambda text: clean_text(text))
    df.drop(columns=["keyword", "location"], inplace=True)
    return df

df.isnull().sum()# missing value check again

df.target.value_counts()# target of the dataset value counts

"""# ● Visualize the distribution of classes (disaster vs. non-disaster tweets) using histograms or bar plots."""

import seaborn as sns
import matplotlib.pyplot as plt

# Set the color palette for the bar plot
sns.set_palette("coolwarm")  # You can also try other palettes like 'viridis', 'magma', etc.

# Create subplots for bar plot and pie chart
fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(15, 4), dpi=100)

# Bar plot for disaster vs non-disaster tweets
sns.barplot(
    x=df['target'].value_counts().index,
    y=df['target'].value_counts().values,
    ax=axes[0],
    palette=['#4CAF50', '#FF5722']  # Custom colors for non-disaster and disaster
)
axes[0].set_title('Bar Plot')
axes[0].set_xlabel('Tweet Type')
axes[0].set_ylabel('Count')

# Pie chart for disaster vs non-disaster tweets
axes[1].pie(
    df['target'].value_counts(),
    labels=['Not Disaster', 'Disaster'],
    autopct='%1.2f%%',
    colors=['#4CAF50', '#FF5722'],  # Matching colors with bar plot
    shadow=True,
    explode=(0.05, 0),  # Slightly separate the first slice
    startangle=60
)
axes[1].set_title('Pie Chart')

# Use suptitle to add a title to the entire figure
fig.suptitle('Disaster vs Non-disaster Tweets', fontsize=24)

# Adjust layout for better display
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Leave space for the title
plt.show()

"""# There are more tweets with class 0 (No disaster) than class 1 (disaster tweets)

● Analyze the frequency of keywords and phrases associated with disaster tweets.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot distribution of classes
sns.countplot(x='target', data=df)
plt.title("Distribution of Classes (0 = Non-Disaster, 1 = Disaster)")
sns.set_palette(['#4CAF50', '#FF5722'])
plt.show()

"""# Insights
Distribution Analysis:

The count of tweets in each class (target) is displayed.
Class 0 (Non-Disaster): Represents tweets that are not related to disasters.
Class 1 (Disaster): Represents tweets that indicate a disasterthe bars for the two classes differ significantly in height, it indicates an imbalance.
"""

# most common keywords
top_keywords = df.keyword.value_counts().iloc[:15]

custom_palette = sns.color_palette("viridis", len(top_keywords))

# plotting
plt.figure(figsize = (9,6))
sns.countplot(y = df.keyword, order = top_keywords.index, palette = custom_palette)
plt.title('Top 15 keywords')
plt.xlabel('Frequency')
plt.ylabel('Keywords')
plt.show()

"""#This is the frequency of keywords and phrases with Disaster tweets"""

disaster = df[df.target == 1].keyword.value_counts().head(10)
non_disaster = df[df.target == 0].keyword.value_counts().head(10)

plt.figure(figsize = (14,4))

# plotting for disaster tweets
plt.subplot(121)
sns.barplot(x = disaster.values, y = disaster.index, color = 'c')
plt.title('Top keywords for disaster tweets')
plt.xlabel('Frequency')
plt.ylabel('Keyword')

# plotting for non-disaster tweets
plt.subplot(122)
sns.barplot(x = non_disaster.values, y = non_disaster.index, color = 'y')
plt.title('Top keywords for non-disaster tweets')
plt.xlabel('Frequency')
plt.ylabel('Keyword')

plt.tight_layout()
plt.show()

df.isnull().sum()# find the missing value from the dataset

# fill missing with 'unknown'
df['keyword'] = df['keyword'].fillna('unknown')
df['location'] = df['location'].fillna('none')

# add keywords to text
df['text'] = df['text'] + ' ' + df['keyword']

# drop id and keyword
columns = ['id', 'keyword']
df = df.drop(columns = columns)

df.isnull().sum()# handle the missing value of the dataset

#filter the dataset by groupby
top_20 = df.groupby('location')['target'].mean().sort_values(ascending=False).head(20)

# creating custom color palette
num_bars = len(top_20)
custom_palette = sns.color_palette('magma', num_bars)

# plotting
plt.figure(figsize=(14,5))
sns.barplot(x=top_20.index, y=top_20.values, palette= custom_palette)
plt.axhline(np.mean(df['target']), color = 'red', linestyle = '--', label = 'Overall Mean')
plt.xticks(rotation=45)
plt.title('Mean Target by Location (Top 20)')
plt.xlabel('Location')
plt.ylabel('Mean Target')
plt.legend()
plt.show()

"""# Insights from the Visualization:
1.High-Impact Locations:

The bar chart highlights the top 20 locations with the highest mean target values, showing which locations are more prone to disaster-related tweets.

2.Comparison with Overall Mean:

The red dashed line allows us to compare each location's mean target value against the overall mean of the dataset.
Locations with bars above the line indicate above-average disaster-related tweet probabilities.

3.Geographical Trends:

Identifying trends in disaster-related tweets by location can provide insights into areas frequently impacted by disasters or areas where people actively tweet about disasters.

4.Data Skew:

If the mean target values for the top 20 locations are significantly higher than the overall mean, it may indicate a skewed dataset with a concentration of disaster-related tweets in specific locations.

5.Actionable Insights:

Disaster response agencies can use this information to focus monitoring and resources in high-impact locations.

# Here we have grouped Target and Location to calculate top 20

Adding more features
"""

df['word count'] = df['text'].apply(lambda x: len(x.split()))

plt.figure(figsize=(10, 4))


# Plot KDE plots for 'word count' based on 'target' values
sns.kdeplot(df['word count'][df['target'] == 0], fill=True,color = 'y', label='Not disaster tweet')
sns.kdeplot(df['word count'][df['target'] == 1], fill=True,color = 'y', label='Disaster tweet')

# Set title, labels, and legend
plt.title('Distribution of Word Count by Tweet Type')
plt.xlabel('Word Count')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()

"""#Disaster tweets are more from 15 to 20 word count category as compared to non disaster tweets"""

df['character count'] = df['text'].apply(lambda x: len(x))

plt.figure(figsize=(10, 4))

# Plot KDE plots for 'word count' based on 'target' values
sns.kdeplot(df['character count'][df['target'] == 0], fill=True,color = 'green', label='Not disaster tweet')
sns.kdeplot(df['character count'][df['target'] == 1], fill=True,color = 'green', label='Disaster tweet')

# Set title, labels, and legend
plt.title('Distribution of Character Count by Tweet Type')
plt.xlabel('Character Count')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()

"""#It tells us that very few disaster tweets are less than 50 characters and that the majority of them are more than 125 characters long"""

# define function to find average word length
def avg_word_length(x):
    x = x.split()
    return np.mean([len(i) for i in x])

df['average word length'] = df['text'].apply(avg_word_length)

plt.figure(figsize=(10, 4))

# Plot KDE plots for 'word count' based on 'target' values
sns.kdeplot(df['average word length'][df['target'] == 0], fill=True, label='Not disaster tweet')
sns.kdeplot(df['average word length'][df['target'] == 1], fill=True, label='Disaster tweet')

# Set title, labels, and legend
plt.title('Distribution of Average Word length by Tweet Type')
plt.xlabel('Average Word Length')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()

import wordcloud

# adding unique word count
df['unique word count'] = df['text'].apply(lambda x: len(set(x.split())))

# adding stopword co
df['stopword count'] = df['text'].apply(lambda x: len([i for i in x.lower().split() if i in wordcloud.STOPWORDS]))

# adding url count
df['url count'] = df['text'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))

# adding mention count
df['mention count'] = df['text'].apply(lambda x: len([i for i in str(x) if i == '@']))

# adding hashtag count
df['hashtag count'] = df['text'].apply(lambda x: len([i for i in str(x) if i == '#']))

# adding stopword ratio
df['stopword ratio'] = df['stopword count'] / df['word count']

# adding punctuation count
df['punctuation count'] = df['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))

df = df[:len(df)]
disaster = df['target'] == 1

# graphs to visualize newly added features
fig, axes = plt.subplots(4, figsize = (12,16))

graph1 = sns.kdeplot(df.loc[~disaster]['unique word count'], fill=True,label = 'Not Disaster', ax=axes[0])
graph1 = sns.kdeplot(df.loc[disaster]['unique word count'], fill=True,label = 'Disasater', ax=axes[0])
graph1.set_title('Distribution of Unique Word Count')

graph2 = sns.kdeplot(df.loc[~disaster]['stopword count'], fill=True, label = 'Not Disaster', ax=axes[1])
graph2 = sns.kdeplot(df.loc[disaster]['stopword count'], fill=True, label = 'Disasater', ax=axes[1])
graph2.set_title('Distribution of StopWord Count')

graph3 = sns.kdeplot(df.loc[~disaster]['punctuation count'], fill=True, label = 'Not Disaster', ax=axes[2], bw=1)
graph3 = sns.kdeplot(df.loc[disaster]['punctuation count'], fill=True, label = 'Disasater', ax=axes[2], bw=1)
graph3.set_title('Distribution of Punctuation Count')

graph4 = sns.kdeplot(df.loc[~disaster]['stopword ratio'], fill=True, label = 'Not Disaster', ax=axes[3], bw=0.5)
graph4 = sns.kdeplot(df.loc[disaster]['stopword ratio'], fill=True, label = 'Disasater', ax=axes[3], bw=0.5)
graph4.set_title('Distribution of Stopword Ratio')

fig.tight_layout()
plt.show()

"""# Insights from Each Graph
(1) Distribution of Unique Word Count
Disaster Tweets:
Likely to have more unique words due to specific terms describing emergencies.
Non-Disaster Tweets:
May have fewer unique words as they might include casual, repetitive vocabulary.
Use Case:
A higher unique word count could be an indicator of disaster-related tweets.

(2) Distribution of Stopword Count
Stopwords (e.g., "is", "and", "the"):
Common in general language but might differ in usage between the two categories.
Disaster Tweets:
May have a slightly different distribution due to urgency and specific keywords.
Non-Disaster Tweets:
Likely to have a more balanced use of stopwords due to casual conversation.
Use Case:
Stopword frequency could help distinguish between formal (disaster) and informal (non-disaster) contexts.

(3) Distribution of Punctuation Count
Punctuation:
Indicates emphasis (e.g., exclamation marks in disaster tweets) or casualness.
Disaster Tweets:
May contain more punctuation for urgency (e.g., "Help!!!").
Non-Disaster Tweets:
Could have less or different usage of punctuation (e.g., for jokes or casual remarks).

Use Case:
High punctuation frequency might signal disaster-related tweets.

(4) Distribution of Stopword Ratio
Stopword Ratio:
Proportion of stopwords to total words.
Disaster Tweets:
Might have a lower ratio due to the use of specific, informative keywords.
Non-Disaster Tweets:
Likely to have a higher ratio as casual tweets tend to be more conversational.
Use Case:
A low stopword ratio could point to more structured, informative tweets (disaster-related).

"""

# Melting the DataFrame to plot multiple count metrics
df_melted = df.melt(id_vars=['target'], value_vars=['url count', 'mention count', 'hashtag count'],
                     var_name='metric', value_name='count')

# Set up the figure and axis
plt.figure(figsize=(10, 5))

# Plotting barplot for each count metric with 'target' as hue
sns.barplot(x='metric', y='count', hue='target', data=df_melted, palette='coolwarm', alpha=1)

# Adding labels and title
plt.xlabel('Metric')
plt.ylabel('Count')
plt.title('Comparison of URL, Mention, and Hashtag Counts by Tweet Type')

# Show the legend
plt.legend(title='Tweet Type (0: Not Disaster, 1: Disaster)')

# Show the plot
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

"""#URL counts in Disaster and non-disaster tweets are maximum followed by Hashtag and Mention count

# Step 1: Clean the Text Data
First, we need to remove unnecessary characters, URLs, punctuation, etc.

# ● Clean the text data by removing special characters, URLs, and punctuation marks.

CLEANING We can do most of the hard work with Keras's Tokenize object, which automatically converts all words to lowercase and filters out punctuation**

This tokenizer has many arguements that allow us to do most of the cleaning with one line of code, so we do not need to much processing ourselves. I have included some examples of how one would manually clean text for reference:
"""

# import required modules
import re
import string
import pandas as pd
from wordcloud import WordCloud, STOPWORDS # Import STOPWORDS
import matplotlib.pyplot as plt
import seaborn as sns

# remove punctuation
def remove_punctuation(x):
    return x.translate(str.maketrans('', '', string.punctuation))

# remove urls
def remove_urls(x):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'', x)

# remove html
def remove_html(x):
    html = re.compile(r'<.*?>')
    return html.sub(r'', x)

# remove emoji
def remove_emoji(text):
    emoji_pattern = re.compile(
        '['
        u'\U0001F600-\U0001F64F'  # emoticons
        u'\U0001F300-\U0001F5FF'  # symbols & pictographs
        u'\U0001F680-\U0001F6FF'  # transport & map symbols
        u'\U0001F1E0-\U0001F1FF'  # flags (iOS)
        u'\U00002702-\U000027B0'
        u'\U000024C2-\U0001F251'
        ']+',
        flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)
# remove stopwords
def remove_stopwords(x):
    return ' '.join([i for i in x.split() if i not in STOPWORDS]) # Access STOPWORDS directly

# remove words less than 4
def remove_less_than(x):
    return ' '.join([i for i in x.split() if len(i) > 3])

# remove words with non-alphabet characters
def remove_non_alphabet(x):
    return ' '.join([i for i in x.split() if i.isalpha()])

def strip_all_entities(x):
    return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x).split())

# Applying helper functions

df['text_clean'] = df['text'].apply(lambda x: remove_urls(x))
df['text_clean'] = df['text_clean'].apply(lambda x: remove_emoji(x))
df['text_clean'] = df['text_clean'].apply(lambda x: remove_html(x))
df['text_clean'] = df['text_clean'].apply(lambda x: remove_punctuation(x))
df['text_clean'] = df['text_clean'].apply(lambda x: remove_stopwords(x))
df['text_clean'] = df['text_clean'].apply(lambda x: remove_less_than(x))
df['text_clean'] = df['text_clean'].apply(lambda x: remove_non_alphabet(x))
df['text_clean'] = df['text_clean'].apply(lambda x: strip_all_entities(x))
df['text_clean'] = df['text_clean'].apply(lambda x: x.lower())
df['text_clean'] = df['text_clean'].apply(lambda x: x.strip())

# checking strip_all_entities function
strip_all_entities('#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'
)

"""#Here we have strip all entities means all the extra things in the sentence have been removed

We can even spellcheck the words
"""

# installing autocorrect
!pip install autocorrect

from autocorrect import Speller

def spell_check(x):
    spell = Speller(lang='en')
    corrected_words = [spell(word) for word in x.split()]
    corrected_sentence = ' '.join(corrected_words)

    return corrected_sentence

wrong_spelled = 'this is my sevanth prooject'
corrected_sentence = spell_check(wrong_spelled)

print('Original Sentence:', wrong_spelled)
print('Corrected Sentence:', corrected_sentence)

"""# With this speller, we can correct the spellings

# ● Tokenize the text into individual words or tokens.
"""

from collections import Counter # Import Counter from collections module
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Filter disaster tweets
disaster_tweets = df[df['target'] == 1]['text']

# Tokenize words and count frequencies
disaster_words = ' '.join(disaster_tweets)
word_freq = Counter(disaster_words.split()) # Now Counter is defined and can be used

# Display the most common words
print(word_freq.most_common(1000))

# Visualize with a WordCloud
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(disaster_words)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# Step 2: Tokenize the Text
After cleaning, tokenize the text into individual words.
"""

import nltk
from nltk.tokenize import word_tokenize

# Download the 'punkt_tab' data package
nltk.download('punkt_tab')

df['tokens'] = df['text'].apply(word_tokenize)
print(df[['text', 'tokens']])

"""# Step 3: Convert Text Labels to Numerical Format
Convert labels like "disaster" and "non-disaster" to numeric format.
"""

df['label'] = df['target'].apply(lambda x: 'disaster' if x == 1 else 'non-disaster')

df['label'] = ['disaster', 'non-disaster'] * (len(df) // 2) + ['disaster'] * (len(df) % 2)

label_map = {'disaster': 1, 'non-disaster': 0}
df['label_num'] = df['label'].map(label_map)
print(df[['label', 'label_num']])

"""# Step 4: Split Dataset into Training and Testing Sets
Finally, split the data for training and testing.
"""

from sklearn.model_selection import train_test_split

# Features and labels
X = df['tokens']
y = df['label_num']

# Split into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", len(X_train))
print("Testing set size:", len(X_test))

"""#Total no of dataset shape is 7613 and 80% of it traninig set and 20% of it testing dataset like traing set  size is 6090 and testing set is 1523."""

# Display the training and testing sets
print("Training set:\n", X_train, "\n", y_train)
print("Testing set:\n", X_test, "\n", y_test)

import nltk
#nltk.download()
from nltk.tokenize import word_tokenize

# Replace the empty append() call with the path you want to add.
nltk.data.path.append('/path/to/your/nltk_data')

import nltk
nltk.download('punkt')

df['tokenized'] = df['text_clean'].apply(lambda x: word_tokenize(x))

from keras.preprocessing.sequence import pad_sequences
from keras import Input
from tensorflow.keras.preprocessing.text import Tokenizer # Import Tokenizer from tensorflow.keras.preprocessing.text

tweets = [tweet for tweet in df['text']]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(tweets)
sequences = tokenizer.texts_to_sequences(tweets)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences)
labels = df['target']
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

# Visualizing tweet lengths with a custom color
plt.figure(figsize=(10, 6))
plt.hist([len(seq) for seq in sequences], bins=50, alpha=0.7, color='orange')  # Changed color to 'orange'
plt.xlabel('Length of Tweets')
plt.ylabel('Frequency')
plt.title('Distribution of Tweet Lengths')
plt.grid(True, linestyle='--', alpha=0.7)  # Dashed gridlines for better visibility
plt.show()

"""# Insights from the Plot
Distribution Shape:

The histogram shows how tweet lengths are distributed across the dataset.

Tweet Length Characteristics:

Tweets with shorter lengths may be casual or informal.
Longer tweets might provide more detailed descriptions, potentially indicating disaster-related information.
Skewness:

The distribution might be skewed:
Left-skewed: More shorter tweets than longer ones.
Right-skewed: A few very long tweets dominate the dataset.
Feature Importance:

Tweet length can serve as a useful feature in the model:
Disaster-related tweets may have more descriptive and longer content.
Non-disaster tweets could be shorter or less structured.




"""

# Visualizing label distribution with a custom color palette
plt.figure(figsize=(6, 4))
labels.value_counts().plot(kind='bar', color=['green', 'red'], alpha=0.7)  # Used green and red for bar colors
plt.xlabel('Labels')
plt.ylabel('Frequency')
plt.title('Distribution of Labels')
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

"""#. Insights from the Visualization
Class Distribution:

The bar chart shows the number of tweets labeled as disasters (1) and non-disasters (0).
Example Scenario:
If one bar is significantly taller than the other, it indicates class imbalance.
Importance of Class Balance:

Class imbalance can negatively affect machine learning models, especially classifiers. For instance:
A heavily imbalanced dataset may lead the model to predict the majority class more often, reducing performance on the minority class.
Guiding Further Actions:

If an imbalance is detected:
Data Augmentation: Generate synthetic data for the minority class.
Resampling Techniques: Oversample the minority class or undersample the majority class.
Class Weights: Assign higher weights to the minority class during model training.
Interpretability and Debugging:

This chart is a quick way to verify if the dataset has roughly equal representation or if specific labels dominate the data.
Useful for debugging dataset preparation.
Visual Appeal:

The use of colors (green and red) makes the chart intuitive:
Green for non-disaster (neutral or safe context).
Red for disaster (alert or dangerous context).

"""

import nltk

# Download the WordNet resource
nltk.download('wordnet')

import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Initialize stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Example function for stemming and lemmatizing a list of tokens
def stem_and_lemmatize(tokens):
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return stemmed_tokens, lemmatized_tokens

# Example usage on your tokenized text column
df['stemmed'], df['lemmatized'] = zip(*df['tokenized'].apply(stem_and_lemmatize))

from wordcloud import WordCloud

# Function to generate word cloud from list of strings
def generate_word_cloud_from_list(text_list, title):
    # Flatten the list of strings
    flattened_text = [word for sublist in text_list for word in sublist]

    # Join all strings in the flattened list into a single string
    text = ' '.join(flattened_text)

    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    # Plot word cloud
    plt.figure(figsize=(10, 4))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Generate word clouds for 'tokenized', 'stemmed', and 'lemmatized' columns
generate_word_cloud_from_list(df['tokenized'], "Word Cloud for Tokenized Text")
generate_word_cloud_from_list(df['stemmed'], "Word Cloud for Stemmed Text")
generate_word_cloud_from_list(df['lemmatized'], "Word Cloud for Lemmatized Text")

"""#  Insights from the Word Clouds
Tokenized Text Word Cloud
Displays the most frequent raw tokens (words) in the dataset.
Useful for understanding the unprocessed dataset.
Common stop words like "the," "is," and "and" may dominate unless they’ve been removed.

Stemmed Text Word Cloud
Highlights stemmed versions of words (e.g., "running" → "run").
Shows a simplified vocabulary by reducing words to their roots.
Helps identify themes and important keywords related to the context (e.g., disaster-related terms).

Lemmatized Text Word Cloud
Displays lemmatized words, which are linguistically meaningful forms (e.g., "children" remains "children," not reduced to "child").
Offers better readability compared to stemmed text.
Useful for identifying semantic patterns in the data.
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

count_vectorizer = CountVectorizer()

X_count = count_vectorizer.fit_transform(df['text'])
feature_names_count = count_vectorizer.get_feature_names_out()
print('Type of X_count:', type(X_count))
print('Shape of X_count:', X_count.shape)
print('Vocabulary (CountVectorizer):\n', feature_names_count)

tfidf_vectorizer = TfidfVectorizer()

X_tfidf = tfidf_vectorizer.fit_transform(df['text'])
feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()
print('Type of X_tfidf:', type(X_tfidf))
print('Shape of X_tfidf:', X_tfidf.shape)
print('Vocabulary (Tfidf-Vectorizer):\n', feature_names_tfidf)

print("Transformed Feature Matrix (CountVectorizer):\n", X_count.toarray())
print("Transformed Feature Matrix (TF-IDF Vectorizer):\n", X_tfidf.toarray())

# Function to plot histogram of word frequencies or TF-IDF scores
def plot_top_features(matrix, feature_names, title, top_n=20):
    # Calculate sum of frequencies/scores for each feature
    feature_sum = matrix.sum(axis=0).tolist()[0]

    # Sort feature names and corresponding sums
    sorted_features = [feature for _, feature in sorted(zip(feature_sum, feature_names), reverse=True)[:top_n]]
    sorted_sums = sorted(feature_sum, reverse=True)[:top_n]

    # Generate a color palette with custom colors
    colors = plt.cm.tab20(range(len(sorted_sums)))  # Use 'tab20' colormap for diverse colors

    # Plot histogram
    plt.figure(figsize=(12, 6))
    plt.bar(range(len(sorted_sums)), sorted_sums, color=colors)  # Add color palette
    plt.xlabel('Features', fontsize=12)
    plt.ylabel('Frequency or TF-IDF Score', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xticks(range(len(sorted_sums)), sorted_features, rotation=45, ha='right', fontsize=10)
    plt.tight_layout()
    plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for better readability
    plt.show()

# Plot top features for CountVectorizer and TF-IDF Vectorizer
plot_top_features(X_count, feature_names_count, 'Top 20 Word Frequencies (CountVectorizer)')
plot_top_features(X_tfidf, feature_names_tfidf, 'Top 20 TF-IDF Scores (TF-IDF Vectorizer)')

"""According to Count Vectorizer co and http are the words which are even more than 4000 in numbers followed by the, in, to, of, and

TF-IDF Vectorizer also shows the same result that co, http are maximum followed by the, in, to, of, and
"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['location_numeric'] = label_encoder.fit_transform(df['location'])

df.head(2)

from sklearn.model_selection import train_test_split

X = df.drop(['target', 'text', 'location'], axis=1)
y = df['target']

# saving dataset for task 2
df.to_csv('twitter_disaster_task2.csv')